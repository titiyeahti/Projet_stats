---
title: "Projet - MST"
author: "Thibaut MILHAUD & Thomas KOWALSKI"
date: "6 mai 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Statistiques descriptives
### Comparaison hommes/femmes

```{r sexism}

data <- read.csv(file = "DB_binome_2.csv");
n <- nrow(data);
mandata <- c();
womandata <- c();
for (i in 1:n)
{
  if(data[i, 'Sexe'] == 0)
  {
    mandata <- c(mandata, data[i, 'Peche'])
  }
  else
  {
    womandata <- c(womandata, data[i, 'Peche'])
  }
}
print(mean(womandata))
print(mean(mandata))
boxplot(mandata, womandata)

```

### Distribution de la pêche en fonction de la tranche d'âge

```{r}
tranches = c(0, 0, 0)
for (i in seq(1, n)) {
    tranche = data[i, "Age"]
    tranches[tranche - 1] = tranches[tranche - 1] + data[i, "Peche"]
}
barplot(tranches)
```

### Intensité du vent

```{r}
hist(data[,'Noeuds'])
```
On remarque que l'intensité du vent est entière, de plus, l'allure de cet histograme rappelle un loi de poisson.


### Quantité de pêche

```{r}
hist(data[,'Peche'])
```
On dirait une loi Normale.

## Statistiques Inférentielles
### Etude du modèle suivi par le vent
#### Cohérence avec la loi de poisson

```{r}
#On regarde la cohérence par rapport à la loi de poisson
a <- seq(0,8,1)
lambda = mean(data[,'Noeuds'])

hist(data[,'Noeuds'],freq=FALSE,breaks = seq(0,8,0.5))
par(new=TRUE)
plot(a,dpois(a,lambda),"l",col="red")



```

#### Vraisemblance

Soit $X$ un echantillon de taille $n$ suivant une loi de poisson de paramêtre $\lambda$, alors sa vraisemblance vaut :
\[
L_\lambda(X) = \prod_{i = 1}^n \exp(-\lambda)\frac{\lambda^{x_i}}{x_i!} = \exp(-n\lambda)\frac{\lambda^{\sum x_i}}{\prod x_i!}
\]
d'où, 
\[
\mathcal L_\lambda(X) = \log(L_\lambda(X)) = -n\lambda + \log \lambda \sum x_i - \sum \log x_i!
\]
Ainsi en dérivant $\mathcal L_\lambda(X)$ par rapport à $\lambda$, on obtient :
\[
{\partial \mathcal L_\lambda(X) \over \partial \lambda } = -n + \frac{\sum x_i}{\lambda}
\]
et 
\[
{\partial^2 \mathcal L_\lambda(X) \over \partial \lambda^2} = -\frac{\sum x_i}{\lambda^2} \leq 0
\]
La log-vraisemblance est donc concave ce qui signifie que les points où la dérivée s'annule sont des maximums globaux. Ainsi,
\[
\lambda_{\max} = {\sum^n_{i=1} x_i \over n}
\]

```{r}
log_L = function(x, l) {
  s = 0
  for(i in seq(1, length(x)))
  {
    s = s + log(factorial(x[i]))
  }
  return(-1 * length(x) * l + log(l) * sum(x) - s)
}

x = seq(0, 10, 0.1)
plot(x = x, 
     y = log_L(data[, "Noeuds"], x),
     main = "Log-Vraisemblance en fonction de lambda",
     xlab = "lambda",
     ylab = "L_lambda(X)",
     type = "l")
lambda = mean(data[,'Noeuds'])
print(lambda)
par(new = TRUE)
abline(v = lambda)
```

### Estimation du paramètre sigma
#### Formules des probabiltés totales

D'après la formule des probabilités totales, on obtient
\[
  P(\text{ pêche} = x) = \sum_{i = 0}^\infty P(\text{ vent} = i) \times \mathbb{P}(\text{ pêche} = x | \text{ vent} = i)
\]
c'est-à-dire que la densité de la loi de $X$ est la suivante :
\[
f_X(x) =  e^{-\lambda} \frac{1}{ \sqrt{2\pi} \sigma }
          \sum_{i=0}^\infty \frac{\lambda^i}{i!}
          \exp\left( -\frac{(x-\frac{100}{1+i})^2}{2\sigma^2} \right)
\]
On peut alors calculer la vraisemblance en faisant le produit des densités evaluées en $x_i$ :
\[
  L_\sigma(x) = \prod^n_{i=1}f_X(x_i)
\]
Cependant l'expression de ce produit ne parait pas se simplifier et on va donc devoir calculer la vraisemblance numériquement puis de trouver son maximum grâce à la fonction \verb{optimize}.
Pour nos applications numériques, on veut trouver $i$ tel que $\mathbb{P}(vent = i) < 0.00001$ (sachant qu'on utilise le $\lambda$ de max vraisemblance trouvé précédemment.) afin de travailler avec des sommes finies.

```{r}
print(lambda)
for(i in seq(1, 100)) {
  p = exp(-1 * lambda) * lambda ^ i / factorial(i)
  if (p < 0.00001)
  {
    break
  }
}
print(i)
```

On trouve donc que la valeur de vent dont la probabilité est inférieure à 0.0000001 est **11**.

Pour déterminer le $\sigma$ de maximum vraisemblance on utilise la fonction `optimize` avec une fonction de vraisemblance que l'on définit. 

```{r}
logvraiss = function(sigma, lambda, X, imax) {
  #initalisation du tableau des puissances de lambda et des i factoriel de 0 à imax
  powers = c(1)
  facts = c(1)
  for(i in 2:(imax+1))
  {
    powers <- c(powers, lambda*powers[i-1])
    facts <- c(facts, (i-1)*facts[i-1])
  }
  logproduit = 0
  for (xi in X) {
    somme = 0
    for (i in i:(imax+1)){
      somme <- somme + powers[i]/facts[i]*exp(-(xi - 100/(i))**2/(2*sigma**2))
    }
    logproduit <- -lambda -log((2*pi)**0.5*sigma) + logproduit + log(somme)
  }
  return(logproduit)
}
```

```{r}
vec = data[, "Peche"]
imax = 11
lambda = lambda # on garde l'ancien
sig = optimise(logvraiss, lambda = lambda, X = vec, imax = imax, lower = 0, upper = 100, maximum = TRUE)
print(sig)
sigma1 = sig$maximum
```

On obtient donc une valeur de $\sigma = 52.1$ en utilisant notre estimateur de vraisemblance approché.

```{r}
Sigma = seq(45,65,0.001)
Logvraissemblance = logvraiss(Sigma,lambda,vec,imax)
plot(x = Sigma, y = Logvraissemblance, type = "l")
par(new = TRUE)
abline(v = sigma1)
```

#### Utilisation du T.C.L pour se ramener à une seule loi

On d'après le TCL on peut se ramener à une seule loi normale $\mathcal{N}(\mu, \sigma^2)$ on va donc utiliser la moyenne empirique pour estimer $\mu$.
D'où notre échantillon va suivre la loi $\mathcal{N}(\overline{X}, \sigma^2)$.


calcul de la vraisemblance :
\[
L = (\frac{1}{\sqrt {2\pi}\sigma})^n\exp(-\frac{\sum{(x_i - \overline{X})^2}}{2\sigma^2})
\]
D'où la log-vraisemblance vaut :
\[
\mathcal{L}(\sigma) = -n \log(\sqrt{2\pi}\sigma) - \frac{1}{2\sigma^2}\sum(x_i - \overline{X})^2
\]
Et
\[
\frac{\partial \mathcal L (\sigma)}{\partial \sigma} = \frac{-n}{\sigma} + \frac{1}{\sigma^3}\sum(x_i - \overline{X})^2
\]
et en cherchant le point où la dérivée s'annule :
\[
\sigma_{\max} = \sqrt{\frac{\sum(x_i - \overline{X})^2}{n}}
\]

```{r calcul}
ecart_type = function (X)
{
  moy = mean(X)
  sum = 0
  for(x in X)
  {
    sum <- sum + (x - moy)**2
  }
  
  return((sum/length(X))**0.5)
}

logvraiss2 <- function(sigma,X)
{
  n = length(X)
  return(-n*log((2*pi)**0.5*sigma) - ecart_type(X)**2*n/(2*sigma**2))
}

sigma2 = ecart_type(vec)
print(sigma2)
Sigma = seq(25,45,0.001)
Logvraissemblance = logvraiss2(Sigma,vec)

plot(x = Sigma, y = Logvraissemblance, type = "l")
par(new = TRUE)
abline(v = sigma2)
```


## Intervalles de confiance
### Calculs théoriques
On sait que :
\[
\Lambda_n=\sqrt n\frac{\overline{X}-\mu}{S_n} \sim  T_{n-1}
\]
où $\overline{X}$ correspond à la moyenne empirique, $\mu$ à l'espérance et $S_n = \frac{1}{n-1}\sum_{i=0}^{n}(x_i - \overline{X})^2$.
D'où, dans notre cas,
\[
\Lambda_n = \sqrt n\frac{\overline{X}-\lambda}{S_n} \sim T_{n-1}
\]
Ainsi, 
\[
\begin{array}{rcl}
\mathbb P(a<\Lambda_n<b) = 1 - \alpha & \Longleftrightarrow & F^{-1}_{T_{n-1}}(\frac \alpha 2) <\Lambda_n < F^{-1}_{T_{n-1}}(1 - \frac \alpha 2) \\
& \Longleftrightarrow & \overline X - \frac{S_n}{\sqrt n} F^{-1}_{T_{n-1}}(1 - \frac{\alpha}{2}) < \lambda < \overline X + \frac{S_n}{\sqrt n} F^{-1}_{T_{n-1}}(1 - \frac \alpha 2) 
\end{array}
\]
### Application numérique
```{r}
confiance <- function(X, alpha=0.05)
{
  n = length(X)
  q = qt(1-alpha/2,n)
  moy = 0
  for (xi in X)
  {
    moy <- moy + xi
  }
  moy <- moy/n
  
  sn = 0
  for (xi in X)
  {
    sn <- sn + (xi-moy)^2
  }
  sn <- sn/(n-1)
  
  q <- sn * q / n**0.5
  return(c(moy-q, moy+q))
}

confiance(data[,"Noeuds"])
```
On obtient que $\lambda$ a 95% de chances d'appartenir à l'intervalle $I = [1.651,1.796]$.

## Tests
### Ultime question

On veut vérifier que lorsque $\lambda = 3$, la QUANTITE DE PECHE suit une loi $\mathcal{N}(\frac{100}{1 + \lambda}, \sigma^2)$ avec $\sigma = 20$. On veut savoir pour quel $\alpha$ on ne peut rejeter cette hypothèse.

On utilise le test $\chi^2$. 

```{r}
peche = c()
for (i in seq(1, n)) {
  if (data[i, "Noeuds"] == 3)
  {
    peche = c(peche, data[i, "Peche"])
  }
}


k = 15
m = min(peche)
M = max(peche)

intervalles = seq(m, M, length.out = k)
print(intervalles)

muChapeau = 0
for(i in seq(1, k - 1))
{
  ci = (intervalles[i] + intervalles[i + 1])
  ni = 0
  for (j in seq(1, length(peche)))
  {
    if (peche[j] >= intervalles[i])
    {
      if(peche[j] < intervalles[i + 1])
      {
        ni = ni + 1
      }
    }
  }
  muChapeau = muChapeau + ci * ni
}
muChapeau = muChapeau / length(peche)

sigmaChapeau = 0
for(i in seq(1, k - 1))
{
  ci = (intervalles[i] + intervalles[i + 1])
  ni = 0
  for (j in seq(1, length(peche)))
  {
    if (peche[j] >= intervalles[i])
    {
      if(peche[j] < intervalles[i + 1])
      {
        ni = ni + 1
      }
    }
  }
  sigmaChapeau = sigmaChapeau + ni * (ci - muChapeau) * (ci - muChapeau)
}
sigmaChapeau = sigmaChapeau / length(peche)
sigmaChapeau = sqrt(sigmaChapeau)

print(muChapeau)
print(sigmaChapeau)

```